# Use LSTM architecture to produce next word given a sequence. 

Trained on Plato's Republic. Example taken from Jason Brownlee (https://machinelearningmastery.com/about/). Due to memory limitations, training on full text is difficult. Will update when I have access to better machine.